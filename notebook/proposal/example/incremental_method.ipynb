{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, TypeVar\n",
    "\n",
    "def process(trainloader, testloader, model, epochs: int, lr: float, lr_scheduling=None, log_savepath=None):\n",
    "\n",
    "    log_dict = defaultdict(list)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    if lr_scheduling is not None:\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_scheduling)\n",
    "\n",
    "    def train(trainloader) -> Tuple[float, float]:\n",
    "        sum_loss, sum_correct, sum_dataN = 0.0, 0, 0\n",
    "        for (inputs, labels) in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            sum_dataN += labels.size(0)\n",
    "            sum_correct += (predicted == labels).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss = sum_loss*trainloader.batch_size/len(trainloader.dataset)\n",
    "        train_acc = float(sum_correct/sum_dataN)\n",
    "        return train_loss, train_acc\n",
    "\n",
    "    def test(testloader) -> Tuple[float, float]:\n",
    "        sum_loss, sum_correct, sum_dataN = 0.0, 0, 0\n",
    "        for (inputs, labels) in testloader:\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            sum_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            sum_dataN += labels.size(0)\n",
    "            sum_correct += (predicted == labels).sum().item()\n",
    "        test_loss = sum_loss*testloader.batch_size/len(testloader.dataset)\n",
    "        test_acc = float(sum_correct/sum_dataN)\n",
    "        return test_loss, test_acc\n",
    "\n",
    "    print(\"\\n{0:<13}{1:<13}{2:<13}{3:<13}{4:<13}{5:<6}\".format(\"epoch\",\"train/loss\",\"train/acc\",\"test/loss\",\"test/acc\",\"lr\"))\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(trainloader)\n",
    "        test_loss, test_acc = test(testloader)\n",
    "        lr = optimizer.param_groups[-1][\"lr\"]\n",
    "        print(\"{0:<13}{1:<13.5f}{2:<13.5f}{3:<13.5f}{4:<13.5f}{5:<6.6f}\".format(epoch, train_loss, train_acc, test_loss, test_acc, lr))\n",
    "        if lr_scheduling is not None: scheduler.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, out=3):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1) # (1) 32*32*3 -> 32*32*16\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1) # (3) 16*16*16 -> 16*16*32\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, padding=1) # (5) 8*8*32 -> 8*8*64\n",
    "        self.fc1 = nn.Linear(4*4*64, 500)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(500, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # (2) 32*32*16 -> 16*16*16\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # (4) 16*16*32 -> 8*8*32\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # (6) 8*8*64 -> 4*4*64\n",
    "        x = x.view(-1, 4*4*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        feature = x\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def download_cifar10(savepath: str):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train = torchvision.datasets.CIFAR10(root=savepath, train=True, download=True, transform=transform)\n",
    "    test = torchvision.datasets.CIFAR10(root=savepath, train=False, download=True, transform=transform)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetPreprocessor:\n",
    "\n",
    "    def __init__(self, train: List[Tuple], test: List[Tuple]):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "    def out_datasets(self, dataframe=False) -> Tuple[List[Tuple], List[Tuple]] or Tuple[Dataframe, Dataframe]:\n",
    "        if dataframe == False:\n",
    "            return self.train, self.test\n",
    "        elif dataframe == True:\n",
    "            train_df = self.__convert_dataframe(self.train)\n",
    "            test_df = self.__convert_dataframe(self.test)\n",
    "            return train_df, test_df\n",
    "\n",
    "    def show_length(self):\n",
    "        print(\"\\n------------------------------------------------------------\")\n",
    "        print(\"Number of train data:{0:>9}\".format(len(self.train)))\n",
    "        print(\"Number of test data:{0:>9}\".format(len(self.test)))\n",
    "\n",
    "    def show_labels(self):\n",
    "        texts = [\"Number of train data\", \"Number of test data\"]\n",
    "        for i, dataset in enumerate([self.train, self.test]):\n",
    "            label_count = defaultdict(int)\n",
    "            for data in dataset:\n",
    "                label_count[data[1]] += 1\n",
    "            print(\"\\n{0}  ----------------------------------------------\".format(texts[i]))\n",
    "            label_count = sorted(label_count.items())\n",
    "            sum = 0\n",
    "            for label, count in label_count:\n",
    "                print(\"label:  {0}    count:  {1}\".format(label, count))\n",
    "                sum += count\n",
    "            print(\"total:  {0}\".format(sum))\n",
    "\n",
    "    # labelsに含まれるラベルのデータを選択\n",
    "    def select_by_label(self, labels: List[int]):\n",
    "        self.train = [data for data in self.train if data[1] in labels]\n",
    "        self.test = [data for data in self.test if data[1] in labels]\n",
    "\n",
    "    # 正解ラベルを0からの連番に更新\n",
    "    def update_labels(self):\n",
    "        updated = [[], []]\n",
    "        label_mapping = defaultdict(lambda: -1)\n",
    "        for i, dataset in enumerate([self.train, self.test]):\n",
    "            dataset = sorted(dataset, key=lambda x:x[1])\n",
    "            new_label = 0\n",
    "            for data in dataset:\n",
    "                if label_mapping[data[1]] == -1:\n",
    "                    label_mapping[data[1]] = new_label\n",
    "                    new_label += 1\n",
    "                updated[i].append((data[0], label_mapping[data[1]]))\n",
    "        self.train, self.test = updated\n",
    "        print(\"\\nChanged the label.  ----------------------------------------------\")\n",
    "        for old, new in label_mapping.items():\n",
    "            print(\"label:  {0} -> {1}\".format(old, new))\n",
    "\n",
    "    # dataset(train, test) をDataFrame形式に変換\n",
    "    def __convert_dataframe(self, dataset: List[Tuple]) -> Dataframe:\n",
    "        dic = defaultdict(list)\n",
    "        for image, label in dataset:\n",
    "            dic[\"image\"].append(json.dumps(image.cpu().numpy().tolist()))\n",
    "            dic[\"label\"].append(int(label))\n",
    "        df = pd.DataFrame(dic)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def select(dataset, dataN, labels=[0,1,2]):\n",
    "    random.shuffle(dataset)\n",
    "    selected = []\n",
    "    dic = defaultdict(list)\n",
    "    for label in labels:\n",
    "        for data in dataset:\n",
    "            if len(dic[label]) >= dataN:\n",
    "                break\n",
    "            if data[1] == label:\n",
    "                dic[label].append(data)\n",
    "        selected += dic[label]\n",
    "        \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "savepath = \"../../../prototype/proposal/data/\"\n",
    "train, test = download_cifar10(savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Changed the label.  ----------------------------------------------\n",
      "label:  1 -> 0\n",
      "label:  2 -> 1\n",
      "label:  8 -> 2\n",
      "\n",
      "Number of train data  ----------------------------------------------\n",
      "label:  0    count:  5000\n",
      "label:  1    count:  5000\n",
      "label:  2    count:  5000\n",
      "total:  15000\n",
      "\n",
      "Number of test data  ----------------------------------------------\n",
      "label:  0    count:  1000\n",
      "label:  1    count:  1000\n",
      "label:  2    count:  1000\n",
      "total:  3000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CLASSES = [1,2,8]\n",
    "preprocessor = DatasetPreprocessor(train, test)\n",
    "preprocessor.select_by_label(TRAIN_CLASSES)\n",
    "preprocessor.update_labels()\n",
    "preprocessor.show_labels()\n",
    "train, test = preprocessor.out_datasets(dataframe=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1回目の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train num: 300\n",
      "test num: 30\n"
     ]
    }
   ],
   "source": [
    "selected_train = select(dataset=train, dataN=100)\n",
    "selected_test = select(dataset=test, dataN=10)\n",
    "print(\"train num: {0}\".format(len(selected_train)))\n",
    "print(\"test num: {0}\".format(len(selected_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 128\n",
    "trainloader = torch.utils.data.DataLoader(selected_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(selected_test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch        train/loss   train/acc    test/loss    test/acc     lr    \n",
      "1            1.40579      0.33667      4.69788      0.26667      0.010000\n",
      "2            1.40737      0.29667      4.68450      0.26667      0.010000\n",
      "3            1.40292      0.38667      4.67562      0.36667      0.010000\n",
      "4            1.40432      0.35000      4.68020      0.40000      0.010000\n",
      "5            1.40171      0.40333      4.67242      0.36667      0.010000\n",
      "6            1.40154      0.39000      4.67798      0.33333      0.010000\n",
      "7            1.39745      0.36667      4.67250      0.40000      0.010000\n",
      "8            1.39708      0.34667      4.65202      0.33333      0.010000\n",
      "9            1.39590      0.35000      4.65504      0.33333      0.010000\n",
      "10           1.38876      0.35667      4.65117      0.33333      0.010000\n"
     ]
    }
   ],
   "source": [
    "model_savepath = \"./assets/test_v1.pth\"\n",
    "model = LeNet(3)\n",
    "model = process(trainloader, testloader, model, epochs=10, lr=0.01)\n",
    "torch.save(model.state_dict(), model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0933, -0.0398,  0.1804],\n",
       "         [ 0.0397, -0.1548,  0.1030],\n",
       "         [-0.1602, -0.0680,  0.1743]],\n",
       "\n",
       "        [[ 0.1842, -0.0073,  0.0016],\n",
       "         [ 0.1044, -0.0353,  0.1169],\n",
       "         [-0.0129, -0.0259,  0.1643]],\n",
       "\n",
       "        [[ 0.0697,  0.1926, -0.0276],\n",
       "         [-0.0641,  0.0099,  0.0959],\n",
       "         [ 0.1730,  0.0037,  0.1823]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[\"conv1.weight\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2回目の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./assets/test_v1.pth\"\n",
    "model2 = LeNet(3)\n",
    "model2.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0933, -0.0398,  0.1804],\n",
       "         [ 0.0397, -0.1548,  0.1030],\n",
       "         [-0.1602, -0.0680,  0.1743]],\n",
       "\n",
       "        [[ 0.1842, -0.0073,  0.0016],\n",
       "         [ 0.1044, -0.0353,  0.1169],\n",
       "         [-0.0129, -0.0259,  0.1643]],\n",
       "\n",
       "        [[ 0.0697,  0.1926, -0.0276],\n",
       "         [-0.0641,  0.0099,  0.0959],\n",
       "         [ 0.1730,  0.0037,  0.1823]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()[\"conv1.weight\"][0] # 前回学習したモデルのパラメータと一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train num: 300\n",
      "test num: 30\n"
     ]
    }
   ],
   "source": [
    "selected_train = select(dataset=train, dataN=100)\n",
    "selected_test = select(dataset=test, dataN=10)\n",
    "print(\"train num: {0}\".format(len(selected_train)))\n",
    "print(\"test num: {0}\".format(len(selected_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 128\n",
    "trainloader = torch.utils.data.DataLoader(selected_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(selected_test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch        train/loss   train/acc    test/loss    test/acc     lr    \n",
      "1            1.39182      0.34333      4.61419      0.33333      0.010000\n",
      "2            1.39002      0.35000      4.60380      0.36667      0.010000\n",
      "3            1.38920      0.35000      4.57252      0.43333      0.010000\n",
      "4            1.39004      0.39000      4.57040      0.43333      0.010000\n",
      "5            1.38041      0.37333      4.57810      0.43333      0.010000\n",
      "6            1.38499      0.38667      4.56418      0.50000      0.010000\n",
      "7            1.37328      0.43333      4.54290      0.50000      0.010000\n",
      "8            1.37653      0.41667      4.47347      0.60000      0.010000\n",
      "9            1.36442      0.48000      4.49608      0.60000      0.010000\n",
      "10           1.34970      0.54000      4.43874      0.56667      0.010000\n"
     ]
    }
   ],
   "source": [
    "model_savepath = \"./assets/test_v2.pth\"\n",
    "model2 = process(trainloader, testloader, model2, epochs=10, lr=0.01)\n",
    "torch.save(model2.state_dict(), model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
