{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット読み込みとdataset_table_indexesの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import sqlite3\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "\n",
    "# datasetをsqliteからDataFrame形式で読み込み\n",
    "def load_dataset(dbpath=\"./ft.db\", tablename=\"feature_table\") -> Dataframe:\n",
    "    conn = sqlite3.connect(dbpath)\n",
    "    c = conn.cursor()\n",
    "    dataset = pd.read_sql('SELECT * FROM ' + tablename, conn)\n",
    "    return dataset\n",
    "\n",
    "def blank_dt_indexes(dt: Dataframe) -> Dict[str, Dict]:\n",
    "    labels = sorted(dt[\"label\"].unique())\n",
    "    dt_indexes = {}\n",
    "    dt_indexes[\"selected_data\"] = {}\n",
    "    for label in labels:\n",
    "        dt_indexes[\"selected_data\"][label] = []\n",
    "    return dt_indexes\n",
    "\n",
    "# feature_table_indexesの初期化 (queryはランダムに選択)\n",
    "def blank_ft_indexes(ft: Dataframe) -> Dict[str, Dict]:\n",
    "    labels = sorted(ft[\"label\"].unique())\n",
    "    ft_indexes = {}\n",
    "    ft_indexes[\"queries\"], ft_indexes[\"used_queries\"], ft_indexes[\"selected_data\"],  = {}, {}, {}\n",
    "    for label in labels:\n",
    "        ft_indexes[\"used_queries\"][label] = []\n",
    "        ft_indexes[\"selected_data\"][label] = []\n",
    "        ft_indexes[\"queries\"][label] = []\n",
    "    return ft_indexes\n",
    "\n",
    "def randomly_init_ft_indexes(ft: Dataframe, queryN=1, seed=0) -> Dict[str, Dict]:\n",
    "    labels = sorted(ft[\"label\"].unique())\n",
    "    ft_labelby = ft.groupby(\"label\")\n",
    "    ft_indexes = {}\n",
    "    ft_indexes[\"queries\"], ft_indexes[\"used_queries\"], ft_indexes[\"selected_data\"],  = {}, {}, {}\n",
    "    for label in labels:\n",
    "        ft_indexes[\"used_queries\"][label] = []\n",
    "        ft_indexes[\"selected_data\"][label] = []\n",
    "        dt = ft_labelby.get_group(label)\n",
    "        dt = dt.sample(n=queryN, random_state=seed)\n",
    "        ft_indexes[\"queries\"][label] = list(dt[\"index\"].values)\n",
    "    return ft_indexes\n",
    "\n",
    "def save_dt_indexes(dt_indexes: Dict[int, Dict], savepath=\"./dt_indexes.json\"):\n",
    "    dic = {}\n",
    "    for k1, v1 in dt_indexes.items():\n",
    "        dic[k1] = {}\n",
    "        for k2, v2 in dt_indexes[k1].items():\n",
    "            dic[k1][str(k2)] = [int(i) for i in v2]\n",
    "    with open(savepath, \"w\") as f:\n",
    "        json.dump(dic, f, indent=4)\n",
    "        \n",
    "def load_dt_indexes(path=\"./dt_indexes.json\"):\n",
    "    dt_indexes = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    for k1, v1 in dic.items():\n",
    "        dt_indexes[k1] = {}\n",
    "        for k2, v2 in dic[k1].items():\n",
    "            dt_indexes[k1][int(k2)] = v2\n",
    "    return dt_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "Tensor = TypeVar(\"torch.Tensor\")\n",
    "NpInt64 = TypeVar(\"numpy.int64\")\n",
    "\n",
    "class DataSelector:\n",
    "    def __init__(self, dt: Dataframe, dt_indexes: Dict[str, Dict[int, List]]):\n",
    "        self.default_dt = dt\n",
    "        self.dt_indexes = copy.deepcopy(dt_indexes)\n",
    "        self.labels = sorted(dt[\"label\"].unique())\n",
    "        # 学習済みのデータを削除したdataset_table\n",
    "        self.dt = self.__init_dt(dt, dt_indexes)\n",
    "        \n",
    "    def __init_dt(self, dt: Dataframe, dt_indexes: Dict[str, Dict[int, List]]) -> Dataframe:\n",
    "        drop_indexes = []\n",
    "        for indexes in dt_indexes[\"selected_data\"].values():\n",
    "            drop_indexes += indexes\n",
    "        dt = dt.drop(index=drop_indexes)\n",
    "        return dt\n",
    "        \n",
    "    def __convert_to_tensor_image(self, json_image) -> Tensor:\n",
    "        image = json.loads(json_image)\n",
    "        image = np.array(image)\n",
    "        image = torch.from_numpy(image.astype(np.float32)).clone()\n",
    "        return image\n",
    "    \n",
    "    def drop_data(self, indexes: List):\n",
    "        self.dt = self.dt.drop(index=indexes)\n",
    "        \n",
    "    def get_dt_indexes(self) -> Dict[str, Dict[int, List]]:\n",
    "        return self.dt_indexes\n",
    "    \n",
    "    def get_dataset(self, indexes_labelby: Dict[int, List]) -> List[Tuple[Tensor, NpInt64]]:\n",
    "        dataset = []\n",
    "        dt_labelby = self.default_dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            indexes = indexes_labelby[label]\n",
    "            dt = dt_labelby.get_group(label)\n",
    "            rows = dt[dt[\"index\"].isin(indexes)]\n",
    "            images = rows[\"image\"].values\n",
    "            labels = rows[\"label\"].values\n",
    "            for image, label in zip(images, labels):\n",
    "                image = self.__convert_to_tensor_image(image)\n",
    "                dataset.append((image, label))\n",
    "        return dataset\n",
    "    \n",
    "    def randomly_select_dt_indexes(self, dataN: int, seed=0) -> Dict[int, List]:\n",
    "        indexes_labelby = {}\n",
    "        dt_labelby = self.dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            dt = dt_labelby.get_group(label)\n",
    "            dt = dt.sample(n=dataN, random_state=seed)\n",
    "            selected_indexes = list(dt[\"index\"].values)\n",
    "            indexes_labelby[label] = selected_indexes\n",
    "            self.dt_indexes[\"selected_data\"][label] += selected_indexes\n",
    "            self.drop_data(selected_indexes)\n",
    "        return indexes_labelby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[0.2549020051956177, 0.45098042488098145, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[0.7019608020782471, 0.6470588445663452, 0.6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 0.9607843160629272, 0.960784...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[[-0.1450980305671692, -0.34117645025253296, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[-0.9843137264251709, -1.0, -1.0, -0.9921568...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              image  label\n",
       "0      0  [[[0.2549020051956177, 0.45098042488098145, 0....      0\n",
       "1      1  [[[0.7019608020782471, 0.6470588445663452, 0.6...      0\n",
       "2      2  [[[1.0, 1.0, 1.0, 0.9607843160629272, 0.960784...      0\n",
       "3      3  [[[-0.1450980305671692, -0.34117645025253296, ...      0\n",
       "4      4  [[[-0.9843137264251709, -1.0, -1.0, -0.9921568...      0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = load_dataset(dbpath=\"./assets/test_dt.db\")\n",
    "print(\"データ数:  {0}\".format(len(dt)))\n",
    "dt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selected_data': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_indexes1 = blank_dt_indexes(dt=dt)\n",
    "dt_indexes1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = DataSelector(dt, dt_indexes1)\n",
    "indexes_labelby1 = selector.randomly_select_dt_indexes(dataN=200, seed=2)\n",
    "indexes_labelby2 = selector.randomly_select_dt_indexes(dataN=200, seed=2)\n",
    "dt_indexes2 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  400\n",
      "ラベル1  の重複なしデータ数:  400\n",
      "ラベル2  の重複なしデータ数:  400\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(dt_indexes2[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = DataSelector(dt, dt_indexes2)\n",
    "indexes_labelby3 = selector.randomly_select_dt_indexes(dataN=200, seed=0)\n",
    "indexes_labelby4 = selector.randomly_select_dt_indexes(dataN=200, seed=0)\n",
    "dt_indexes3 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  800\n",
      "ラベル1  の重複なしデータ数:  800\n",
      "ラベル2  の重複なしデータ数:  800\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(dt_indexes3[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  2400\n",
      "(tensor([[[ 0.2549,  0.4510,  0.6392,  ..., -0.2078, -0.2863, -0.2627],\n",
      "         [ 0.7647,  0.8745,  0.8980,  ..., -0.1608, -0.2392, -0.2157],\n",
      "         [ 0.9765,  0.9529,  0.8902,  ..., -0.0980, -0.1765, -0.1765],\n",
      "         ...,\n",
      "         [-0.3725, -0.3020, -0.3647,  ..., -0.9137, -0.8353, -0.7412],\n",
      "         [-0.4275, -0.3804, -0.4275,  ..., -0.8824, -0.7882, -0.7569],\n",
      "         [-0.4588, -0.4353, -0.4039,  ..., -0.7961, -0.7647, -0.7725]],\n",
      "\n",
      "        [[-0.7098, -0.6157, -0.5529,  ..., -0.8588, -0.8353, -0.8510],\n",
      "         [-0.4745, -0.4353, -0.3961,  ..., -0.8353, -0.8275, -0.8588],\n",
      "         [-0.4353, -0.4667, -0.4353,  ..., -0.8431, -0.8510, -0.8667],\n",
      "         ...,\n",
      "         [-0.7412, -0.7098, -0.6941,  ..., -0.8902, -0.8118, -0.7725],\n",
      "         [-0.7255, -0.7098, -0.6784,  ..., -0.8431, -0.7569, -0.7647],\n",
      "         [-0.6706, -0.6706, -0.5373,  ..., -0.7804, -0.7490, -0.7647]],\n",
      "\n",
      "        [[-0.8980, -0.9137, -0.8902,  ..., -0.9373, -0.9529, -0.9608],\n",
      "         [-0.8980, -0.9059, -0.9216,  ..., -0.9373, -0.9686, -0.9529],\n",
      "         [-0.9451, -0.9686, -0.9765,  ..., -0.9529, -0.9765, -0.9529],\n",
      "         ...,\n",
      "         [-0.7412, -0.6863, -0.6549,  ..., -0.9137, -0.8353, -0.7804],\n",
      "         [-0.7098, -0.6784, -0.6235,  ..., -0.8588, -0.7725, -0.7804],\n",
      "         [-0.6549, -0.6157, -0.4510,  ..., -0.7569, -0.7333, -0.7804]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = selector.get_dataset(dt_indexes3[\"selected_data\"])\n",
    "print(\"データ数:  {0}\".format(len(dataset)))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dt_indexes(dt_indexes=dt_indexes3, savepath=\"./assets/dt_indexes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['selected_data'])\n",
      "dict_keys([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "dt_indexes3 = load_dt_indexes(path=\"assets/dt_indexes.json\")\n",
    "print(dt_indexes3.keys())\n",
    "print(dt_indexes3[\"selected_data\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "Tensor = TypeVar(\"torch.Tensor\")\n",
    "NpArrayFloat32 = TypeVar(\"numpy.ndarray.float32\")\n",
    "NpInt64 = TypeVar(\"numpy.int64\")\n",
    "FaissIndexFlatL2 = TypeVar(\"faiss.swigfaiss.IndexFlatL2\")\n",
    "\n",
    "class FeatureSelector(DataSelector):\n",
    "    def __init__(self, ft: Dataframe, ft_indexes: Dict[str, Dict[int, List]]):\n",
    "        super().__init__(ft, ft_indexes)\n",
    "        \n",
    "    def __ft_to_features(self, ft: Dataframe) -> NpArrayFloat32:\n",
    "        features = [json.loads(f) for f in ft[\"feature\"]]\n",
    "        features = np.array(features).astype(\"float32\")\n",
    "        return features\n",
    "    \n",
    "    def __indexes_to_features(self, ft: Dataframe, indexes: List[int]) -> NpArrayFloat32:\n",
    "        features = []\n",
    "        for index in indexes:\n",
    "            feature = ft[ft[\"index\"] == index][\"feature\"].iloc[0]\n",
    "            feature = json.loads(feature)\n",
    "            features.append(feature)\n",
    "        features = np.array(features).astype(\"float32\")\n",
    "        if len(features) != len(indexes): print(\"There is a feature that cannot be obtained\")\n",
    "        return features\n",
    "            \n",
    "    def __generate_faiss_index(self, vectors: NpArrayFloat32) -> FaissIndexFlatL2:\n",
    "        dim = len(vectors[0])\n",
    "        faiss_index = faiss.IndexFlatL2(dim)\n",
    "        faiss_index.add(vectors)\n",
    "        return faiss_index\n",
    "    \n",
    "    def __search_NN_ft_indexes(self, ft: Dataframe, query_ft_indexes: List[int], dataN: int) -> List[int]:\n",
    "        queries = self.__indexes_to_features(ft, query_ft_indexes)\n",
    "        features = self.__ft_to_features(ft)\n",
    "        faiss_index = self.__generate_faiss_index(features)\n",
    "        k = faiss_index.ntotal # 検索対象データ数\n",
    "        D, I = faiss_index.search(queries, k) # 近傍探索\n",
    "        \n",
    "        NN_ft_indexes = []\n",
    "        all_query_indexes = [index for indexes in self.dt_indexes[\"queries\"].values() for index in indexes]\n",
    "        for indexes in I:\n",
    "            cnt, i = 0, 0\n",
    "            while cnt < dataN:\n",
    "                ft_index = ft.iloc[indexes[i]][\"index\"]\n",
    "                i += 1\n",
    "                if ft_index in NN_ft_indexes: continue # 既に選択済みのインデックスは検索対象外\n",
    "                if ft_index in all_query_indexes: continue # クエリは検索対象外\n",
    "                NN_ft_indexes.append(ft_index)\n",
    "                cnt += 1\n",
    "                \n",
    "        return NN_ft_indexes\n",
    "    \n",
    "    def __search_FP_ft_indexes(self, ft: Dataframe, query_ft_indexes: List[int]) -> List[int]:\n",
    "        queries = self.__indexes_to_features(ft, query_ft_indexes)\n",
    "        features = self.__ft_to_features(ft)\n",
    "        faiss_index = self.__generate_faiss_index(features)\n",
    "        k = faiss_index.ntotal # 検索対象データ数\n",
    "        D, I = faiss_index.search(queries, k) # 近傍探索\n",
    "        \n",
    "        FP_ft_indexes = []\n",
    "        all_used_query_indexes = [index for indexes in self.dt_indexes[\"used_queries\"].values() for index in indexes]\n",
    "        for indexes in I:\n",
    "            for index in reversed(indexes):\n",
    "                ft_index = ft.iloc[index][\"index\"]\n",
    "                if ft_index in FP_ft_indexes: continue # 既に選択済みのインデックスは検索対象外\n",
    "                if ft_index not in all_used_query_indexes: break # 一度でも使用されたクエリは検索対象外\n",
    "            FP_ft_indexes.append(ft_index)\n",
    "        \n",
    "        return FP_ft_indexes\n",
    "    \n",
    "    def __search_ft_indexes_with_rate(self, ft: Dataframe, query_ft_indexes: List[int], rate=1/2) -> List[int]:\n",
    "        queries = self.__indexes_to_features(ft, query_ft_indexes)\n",
    "        features = self.__ft_to_features(ft)\n",
    "        faiss_index = self.__generate_faiss_index(features)\n",
    "        k = faiss_index.ntotal # 検索対象データ数\n",
    "        D, I = faiss_index.search(queries, k) # 近傍探索\n",
    "        \n",
    "        MP_ft_indexes=[]\n",
    "        all_used_query_indexes = [index for indexes in self.dt_indexes[\"used_queries\"].values() for index in indexes]\n",
    "        for indexes in I:\n",
    "            i = round(k*rate)\n",
    "            while (1):\n",
    "                index = indexes[i]\n",
    "                ft_index = ft.iloc[index][\"index\"]\n",
    "                i += 1\n",
    "                if ft_index in MP_ft_indexes: continue # 既に選択済みのインデックスは検索対象外\n",
    "                if ft_index not in all_used_query_indexes: break # 一度でも使用されたクエリは検索対象外\n",
    "            MP_ft_indexes.append(ft_index)\n",
    "        \n",
    "        return MP_ft_indexes\n",
    "    \n",
    "    def init_ft_indexes(self, queryN=1, seed=0) -> Dict[str, Dict[int, List]]:\n",
    "        ft_indexes = {}\n",
    "        ft_indexes[\"queries\"], ft_indexes[\"used_queries\"], ft_indexes[\"selected_data\"],  = {}, {}, {}\n",
    "        ft_labelby = self.default_dt.groupby(\"label\")\n",
    "        \n",
    "        for label in self.labels:\n",
    "            ft_indexes[\"selected_data\"][label] = []\n",
    "            ft_indexes[\"used_queries\"][label] = []\n",
    "            ft = ft_labelby.get_group(label)\n",
    "            query = json.loads(ft.sample(n=1, random_state=seed)[\"feature\"].iloc[0])\n",
    "            query = np.array([query]).astype(\"float32\")\n",
    "            features = self.__ft_to_features(ft)\n",
    "            faiss_index = self.__generate_faiss_index(features)\n",
    "            k = faiss_index.ntotal\n",
    "            linspace = k//queryN\n",
    "            D, I = faiss_index.search(query, k)\n",
    "            \n",
    "            indexes = I[0][::linspace][:queryN]\n",
    "            tmp_ft_indexes = []\n",
    "            for index in indexes:\n",
    "                ft_index = ft.iloc[index][\"index\"]\n",
    "                tmp_ft_indexes.append(ft_index)\n",
    "            ft_indexes[\"queries\"][label] = tmp_ft_indexes\n",
    "        \n",
    "        return ft_indexes\n",
    "                \n",
    "    def select_NN_ft_indexes(self, dataN: int) -> Dict[int, List]:\n",
    "        indexes_labelby = {}\n",
    "        ft_labelby = self.dt.groupby(\"label\")\n",
    "        \n",
    "        for label in self.labels:\n",
    "            ft = ft_labelby.get_group(label)\n",
    "            query_ft_indexes = self.dt_indexes[\"queries\"][label]\n",
    "            NN_ft_indexes = self.__search_NN_ft_indexes(ft, query_ft_indexes, dataN)\n",
    "            \n",
    "            indexes_labelby[label] = NN_ft_indexes\n",
    "            self.dt_indexes[\"selected_data\"][label] += NN_ft_indexes\n",
    "            self.dt_indexes[\"used_queries\"][label] += query_ft_indexes\n",
    "            self.drop_data(NN_ft_indexes)\n",
    "            \n",
    "        return indexes_labelby\n",
    "    \n",
    "    # クエリをFP(最遠傍点)へ更新\n",
    "    def update_to_FP_queries(self) -> Dict[int, List]:\n",
    "        indexes_labelby = {}\n",
    "        ft_labelby = self.dt.groupby(\"label\")\n",
    "        \n",
    "        for label in self.labels:\n",
    "            ft = ft_labelby.get_group(label)\n",
    "            query_ft_indexes = self.dt_indexes[\"queries\"][label]\n",
    "            FP_ft_indexes = self.__search_FP_ft_indexes(ft, query_ft_indexes)\n",
    "            \n",
    "            indexes_labelby[label] = FP_ft_indexes\n",
    "            self.dt_indexes[\"queries\"][label] = FP_ft_indexes\n",
    "            \n",
    "        return indexes_labelby\n",
    "    \n",
    "    # クエリを指定割合だけ更新\n",
    "    def update_queries(self, rate=1/2) -> Dict[int, List]:\n",
    "        indexes_labelby = {}\n",
    "        ft_labelby = self.dt.groupby(\"label\")\n",
    "        \n",
    "        for label in self.labels:\n",
    "            ft = ft_labelby.get_group(label)\n",
    "            query_ft_indexes = self.dt_indexes[\"queries\"][label]\n",
    "            ft_indexes = self.__search_ft_indexes_with_rate(ft, query_ft_indexes, rate)\n",
    "            \n",
    "            indexes_labelby[label] = ft_indexes\n",
    "            self.dt_indexes[\"queries\"][label] = ft_indexes\n",
    "            \n",
    "        return indexes_labelby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feature</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.30711856484413147, 0.19312363862991333, 0.0...</td>\n",
       "      <td>[[[-0.3176470398902893, -0.29411762952804565, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4214461147785187, 1.198604702949524, 0.9510...</td>\n",
       "      <td>[[[-0.9764705896377563, -0.9686274528503418, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.2851516008377075, 0.20933431386947632, 0.07...</td>\n",
       "      <td>[[[0.13725495338439941, 0.13725495338439941, 0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.6752024292945862, 0.7612708806991577, 0.712...</td>\n",
       "      <td>[[[-0.24705880880355835, -0.27843135595321655,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.3068203926086426, 0.6951863169670105, 0.444...</td>\n",
       "      <td>[[[0.30980396270751953, 0.30980396270751953, 0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            feature  \\\n",
       "0      0  [0.30711856484413147, 0.19312363862991333, 0.0...   \n",
       "1      1  [0.4214461147785187, 1.198604702949524, 0.9510...   \n",
       "2      2  [0.2851516008377075, 0.20933431386947632, 0.07...   \n",
       "3      3  [0.6752024292945862, 0.7612708806991577, 0.712...   \n",
       "4      4  [0.3068203926086426, 0.6951863169670105, 0.444...   \n",
       "\n",
       "                                               image  label  \n",
       "0  [[[-0.3176470398902893, -0.29411762952804565, ...      1  \n",
       "1  [[[-0.9764705896377563, -0.9686274528503418, -...      1  \n",
       "2  [[[0.13725495338439941, 0.13725495338439941, 0...      2  \n",
       "3  [[[-0.24705880880355835, -0.27843135595321655,...      2  \n",
       "4  [[[0.30980396270751953, 0.30980396270751953, 0...      2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = load_dataset(dbpath=\"./assets/ft.db\")\n",
    "print(\"データ数:  {0}\".format(len(ft)))\n",
    "ft[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {0: [8389, 14320, 11549, 10570, 8310],\n",
       "  1: [8221, 14297, 11383, 10409, 8145],\n",
       "  2: [8261, 14291, 11433, 10524, 8169]},\n",
       " 'selected_data': {0: [], 1: [], 2: []},\n",
       " 'used_queries': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_indexes1 = randomly_init_ft_indexes(ft=ft, queryN=5, seed=1)\n",
    "ft_indexes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {0: [8389, 278, 3498, 4078, 3161],\n",
       "  1: [8221, 14026, 5401, 4736, 845],\n",
       "  2: [8261, 2578, 10993, 8561, 1692]},\n",
       " 'selected_data': {0: [], 1: [], 2: []},\n",
       " 'used_queries': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_ft_indexes1 = blank_ft_indexes(ft=ft)\n",
    "selector = FeatureSelector(ft=ft, ft_indexes=blank_ft_indexes1)\n",
    "ft_indexes1 = selector.init_ft_indexes(queryN=5, seed=1)\n",
    "ft_indexes1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selector = FeatureSelector(ft, ft_indexes1)\n",
    "indexes_labelby1 = selector.select_NN_ft_indexes(dataN=50)\n",
    "indexes_labelby2 = selector.select_NN_ft_indexes(dataN=50)\n",
    "ft_indexes2 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  500\n",
      "ラベル1  の重複なしデータ数:  500\n",
      "ラベル2  の重複なしデータ数:  500\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes2[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(ft, ft_indexes2)\n",
    "indexes_labelby3 = selector.select_NN_ft_indexes(dataN=50)\n",
    "indexes_labelby4 = selector.select_NN_ft_indexes(dataN=50)\n",
    "ft_indexes3 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  1000\n",
      "ラベル1  の重複なしデータ数:  1000\n",
      "ラベル2  の重複なしデータ数:  1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes3[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(ft, ft_indexes3)\n",
    "indexes_labelby5 = selector.update_to_FP_queries() # クエリの更新\n",
    "indexes_labelby6 = selector.select_NN_ft_indexes(dataN=100)\n",
    "ft_indexes4 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  1500\n",
      "ラベル1  の重複なしデータ数:  1500\n",
      "ラベル2  の重複なしデータ数:  1500\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes4[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1347, 2238, 2070, 8954, 4165], 1: [7522, 6188, 14867, 5218, 14887], 2: [916, 14267, 3882, 10627, 9697]}\n",
      "{0: [4353, 8787, 107, 11956, 10470], 1: [8833, 10055, 5380, 4324, 5368], 2: [1609, 10722, 3546, 4383, 6056]}\n"
     ]
    }
   ],
   "source": [
    "print(ft_indexes3[\"queries\"])\n",
    "print(ft_indexes4[\"queries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  4500\n",
      "(tensor([[[1.0000, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 0.9843, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 1.0000, 0.9922, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9843,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 0.9843, 0.9451,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 1.0000, 0.9922, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9843,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 0.9843, 0.9922, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 0.9843, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.9843,  ..., 1.0000, 1.0000, 1.0000]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = selector.get_dataset(ft_indexes4[\"selected_data\"])\n",
    "print(\"データ数:  {0}\".format(len(dataset)))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_to_MP_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {0: [10765, 12835, 5776, 12445, 4431],\n",
       "  1: [10596, 12717, 5751, 12319, 4413],\n",
       "  2: [10741, 12690, 5740, 12227, 4400]},\n",
       " 'selected_data': {0: [], 1: [], 2: []},\n",
       " 'used_queries': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_indexes1 = randomly_init_ft_indexes(ft=ft, queryN=5, seed=2)\n",
    "ft_indexes1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(ft, ft_indexes1)\n",
    "indexes_labelby1 = selector.update_queries(rate=1/2)\n",
    "indexes_labelby2 = selector.select_NN_ft_indexes(dataN=50)\n",
    "ft_indexes2 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  250\n",
      "ラベル1  の重複なしデータ数:  250\n",
      "ラベル2  の重複なしデータ数:  250\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes2[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(ft, ft_indexes2)\n",
    "indexes_labelby3 = selector.update_queries(rate=1/4)\n",
    "indexes_labelby4 = selector.select_NN_ft_indexes(dataN=50)\n",
    "ft_indexes3 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  500\n",
      "ラベル1  の重複なしデータ数:  500\n",
      "ラベル2  の重複なしデータ数:  500\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes3[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
