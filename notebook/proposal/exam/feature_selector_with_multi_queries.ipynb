{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット読み込みとfeature_table_indexesの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "import sqlite3\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "\n",
    "# datasetをsqliteからDataFrame形式で読み込み\n",
    "def load_dataset(dbpath=\"./ft.db\", tablename=\"feature_table\") -> Dataframe:\n",
    "    conn=sqlite3.connect(dbpath)\n",
    "    c = conn.cursor()\n",
    "    dataset = pd.read_sql('SELECT * FROM ' + tablename, conn)\n",
    "    return dataset\n",
    "\n",
    "# feature_table_indexesの初期化 (queryはランダムに選択)\n",
    "def init_ft_indexes(ft: Dataframe, queryN=1, seed=0) -> Dict[str, Dict]:\n",
    "    random.seed(seed)\n",
    "    labels = sorted(ft[\"label\"].unique())\n",
    "    ft_labelby = ft.groupby(\"label\")\n",
    "    ft_indexes = {}\n",
    "    ft_indexes[\"queries\"], ft_indexes[\"used_queries\"], ft_indexes[\"selected_data\"],  = {}, {}, {}\n",
    "    \n",
    "    for label in labels:\n",
    "        ft_indexes[\"used_queries\"][label] = []\n",
    "        ft_indexes[\"selected_data\"][label] = []\n",
    "        \n",
    "        ft = ft_labelby.get_group(label)\n",
    "        indexes = ft[\"index\"].values.tolist()\n",
    "        queries = random.sample(indexes, queryN)\n",
    "        ft_indexes[\"queries\"][label] = queries\n",
    "    \n",
    "    return ft_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "Tensor = TypeVar(\"torch.Tensor\")\n",
    "NpInt64 = TypeVar(\"numpy.int64\")\n",
    "\n",
    "class DataSelector:\n",
    "    def __init__(self, dt: Dataframe, dt_indexes: Dict[str, Dict[int, List]]):\n",
    "        self.default_dt = dt\n",
    "        self.dt_indexes = copy.deepcopy(dt_indexes)\n",
    "        self.labels = sorted(dt[\"label\"].unique())\n",
    "        # 学習済みのデータを削除したdataset_table\n",
    "        self.dt = self.__init_dt(dt, dt_indexes)\n",
    "        \n",
    "    def __init_dt(self, dt: Dataframe, dt_indexes: Dict[str, Dict[int, List]]) -> Dataframe:\n",
    "        drop_indexes = []\n",
    "        for indexes in dt_indexes[\"selected_data\"].values():\n",
    "            drop_indexes += indexes\n",
    "        dt = dt.drop(index=drop_indexes)\n",
    "        return dt\n",
    "    \n",
    "    def __drop_data(self, indexes: List):\n",
    "        self.dt = self.dt.drop(index=indexes)\n",
    "        \n",
    "    def __convert_to_tensor_image(self, json_image) -> Tensor:\n",
    "        image = json.loads(json_image)\n",
    "        image = np.array(image)\n",
    "        image = torch.from_numpy(image.astype(np.float32)).clone()\n",
    "        return image\n",
    "        \n",
    "    def get_dt_indexes(self) -> Dict[str, Dict[int, List]]:\n",
    "        return self.dt_indexes\n",
    "    \n",
    "    def get_dataset(self, indexes_labelby: Dict[int, List]) -> List[Tuple[Tensor, NpInt64]]:\n",
    "        dataset = []\n",
    "        dt_labelby = self.default_dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            indexes = indexes_labelby[label]\n",
    "            dt = dt_labelby.get_group(label)\n",
    "            rows = dt[dt[\"index\"].isin(indexes)]\n",
    "            images = rows[\"image\"].values\n",
    "            labels = rows[\"label\"].values\n",
    "            for image, label in zip(images, labels):\n",
    "                image = self.__convert_to_tensor_image(image)\n",
    "                dataset.append((image, label))\n",
    "        return dataset\n",
    "    \n",
    "    def randomly_select_dt_indexes(self, dataN: int, seed=0) -> Dict[int, List]:\n",
    "        indexes_labelby = {}\n",
    "        dt_labelby = self.dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            dt = dt_labelby.get_group(label)\n",
    "            dt = dt.sample(n=dataN, random_state=seed)\n",
    "            selected_indexes = list(dt[\"index\"].values)\n",
    "            indexes_labelby[label] = selected_indexes\n",
    "            self.dt_indexes[\"selected_data\"][label] += selected_indexes\n",
    "            self.__drop_data(selected_indexes)\n",
    "        return indexes_labelby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feature</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.30711856484413147, 0.19312363862991333, 0.0...</td>\n",
       "      <td>[[[-0.3176470398902893, -0.29411762952804565, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4214461147785187, 1.198604702949524, 0.9510...</td>\n",
       "      <td>[[[-0.9764705896377563, -0.9686274528503418, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.2851516008377075, 0.20933431386947632, 0.07...</td>\n",
       "      <td>[[[0.13725495338439941, 0.13725495338439941, 0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.6752024292945862, 0.7612708806991577, 0.712...</td>\n",
       "      <td>[[[-0.24705880880355835, -0.27843135595321655,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.3068203926086426, 0.6951863169670105, 0.444...</td>\n",
       "      <td>[[[0.30980396270751953, 0.30980396270751953, 0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            feature  \\\n",
       "0      0  [0.30711856484413147, 0.19312363862991333, 0.0...   \n",
       "1      1  [0.4214461147785187, 1.198604702949524, 0.9510...   \n",
       "2      2  [0.2851516008377075, 0.20933431386947632, 0.07...   \n",
       "3      3  [0.6752024292945862, 0.7612708806991577, 0.712...   \n",
       "4      4  [0.3068203926086426, 0.6951863169670105, 0.444...   \n",
       "\n",
       "                                               image  label  \n",
       "0  [[[-0.3176470398902893, -0.29411762952804565, ...      1  \n",
       "1  [[[-0.9764705896377563, -0.9686274528503418, -...      1  \n",
       "2  [[[0.13725495338439941, 0.13725495338439941, 0...      2  \n",
       "3  [[[-0.24705880880355835, -0.27843135595321655,...      2  \n",
       "4  [[[0.30980396270751953, 0.30980396270751953, 0...      2  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = load_dataset(dbpath=\"./assets/ft.db\")\n",
    "print(\"データ数:  {0}\".format(len(ft)))\n",
    "ft[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {0: [1347, 2238, 2070, 8954, 4165],\n",
       "  1: [7522, 6188, 14867, 5218, 14887],\n",
       "  2: [916, 14267, 3882, 10627, 9697]},\n",
       " 'selected_data': {0: [], 1: [], 2: []},\n",
       " 'used_queries': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_indexes1 = init_ft_indexes(ft=ft, queryN=5, seed=2)\n",
    "ft_indexes1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = DataSelector(ft, ft_indexes1)\n",
    "indexes_labelby1 = selector.randomly_select_dt_indexes(dataN=200, seed=2)\n",
    "indexes_labelby2 = selector.randomly_select_dt_indexes(dataN=200, seed=2)\n",
    "ft_indexes2 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  400\n",
      "ラベル1  の重複なしデータ数:  400\n",
      "ラベル2  の重複なしデータ数:  400\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes2[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2反復目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = DataSelector(ft, ft_indexes2)\n",
    "indexes_labelby3 = selector.randomly_select_dt_indexes(dataN=200, seed=0)\n",
    "indexes_labelby4 = selector.randomly_select_dt_indexes(dataN=200, seed=0)\n",
    "ft_indexes3 = selector.get_dt_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラベル0  の重複なしデータ数:  800\n",
      "ラベル1  の重複なしデータ数:  800\n",
      "ラベル2  の重複なしデータ数:  800\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"ラベル{0}  の重複なしデータ数:  {1}\".format(i, len(set(ft_indexes3[\"selected_data\"][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  2400\n",
      "(tensor([[[-0.6471, -0.6000, -0.6078,  ..., -0.6392, -0.6000, -0.7412],\n",
      "         [-0.7020, -0.6078, -0.5137,  ..., -0.7255, -0.6235, -0.6627],\n",
      "         [-0.6863, -0.5451, -0.5608,  ..., -0.7098, -0.5216, -0.5216],\n",
      "         ...,\n",
      "         [ 0.5373,  0.4980,  0.4510,  ...,  0.8431,  0.9373,  0.7569],\n",
      "         [ 0.6157,  0.6784,  0.6863,  ...,  0.8745,  0.6863,  0.4275],\n",
      "         [ 0.4431,  0.4510,  0.5216,  ...,  0.6235,  0.4039,  0.3647]],\n",
      "\n",
      "        [[-0.6314, -0.5529, -0.6078,  ..., -0.6235, -0.6157, -0.6627],\n",
      "         [-0.6314, -0.5373, -0.5294,  ..., -0.7020, -0.6471, -0.6549],\n",
      "         [-0.6235, -0.4745, -0.5686,  ..., -0.6863, -0.5373, -0.5373],\n",
      "         ...,\n",
      "         [-0.8667, -0.8118, -0.7804,  ...,  0.3804,  0.2314, -0.5451],\n",
      "         [-0.9529, -0.9216, -0.7569,  ...,  0.8431,  0.1373, -0.6549],\n",
      "         [-0.7255, -0.7882, -0.5451,  ...,  0.3961, -0.0588, -0.1529]],\n",
      "\n",
      "        [[-0.6627, -0.6314, -0.7020,  ..., -0.6000, -0.6471, -0.6706],\n",
      "         [-0.7255, -0.6235, -0.5765,  ..., -0.6863, -0.6706, -0.6471],\n",
      "         [-0.7490, -0.5765, -0.6314,  ..., -0.6784, -0.5451, -0.5216],\n",
      "         ...,\n",
      "         [-0.6941, -0.6235, -0.6863,  ...,  0.5137,  0.4510, -0.3333],\n",
      "         [-0.8039, -0.6706, -0.5137,  ...,  0.8431,  0.2471, -0.5451],\n",
      "         [-0.6863, -0.6314, -0.3647,  ...,  0.4510, -0.0039, -0.1686]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = selector.get_dataset(ft_indexes3[\"selected_data\"])\n",
    "print(\"データ数:  {0}\".format(len(dataset)))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(DataSelector):\n",
    "    def __init__(self, feature_table: Dataframe, feature_table_indexes: Dict[str, List[int]]):\n",
    "        super().__init__(feature_table, feature_table_indexes)\n",
    "        self.faiss_indexes = {} # ラベルごとのフィーチャ全体のfaissインデックス\n",
    "\n",
    "    # フィーチャを検索するための,フィーチャ全体のfaissインデックスを作成\n",
    "    def make_faiss_indexes(self):\n",
    "        dt_labelby = self.dropped_dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            features = []\n",
    "            df = dt_labelby.get_group(label)\n",
    "            for feature in df[\"feature\"]:\n",
    "                features.append(json.loads(feature))\n",
    "            features = np.array(features).astype(\"float32\")\n",
    "            dim = len(features[0])\n",
    "            index = faiss.IndexFlatL2(dim)\n",
    "            index.add(features)\n",
    "            self.faiss_indexes[label] = index\n",
    "\n",
    "    # ラベルごとにクエリと最近傍(NN)のフィーチャをdataN分選択し、選択したフィーチャのdt_indexesをdt_indexes[\"selected\"]に追加\n",
    "    def add_NN(self, dataN: int) -> Tuple[Dict[str, List[int]], List[int]]:\n",
    "        if len(self.faiss_indexes) is 0:\n",
    "            print(\"\\nPlease run the process to make faiss indexes in advance.\")\n",
    "            sys.exit()\n",
    "        selected_indexes = []\n",
    "        dt_labelby = self.dropped_dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            index = self.faiss_indexes[label]\n",
    "            k = index.ntotal # 検索対象データ数\n",
    "            query = self.dropped_dt[self.dropped_dt[\"index\"]==self.dt_indexes[\"query\"][label]][\"feature\"].iat[0] # queryに指定されたフィーチャを取得\n",
    "            query = json.loads(query)\n",
    "            query = np.array([query]).astype(\"float32\")\n",
    "            D, I = index.search(query, k)\n",
    "            self.dt_indexes[\"selected_query\"].append(self.dt_indexes[\"query\"][label])\n",
    "            for i in I[0][:dataN + 1]:\n",
    "                train_index = dt_labelby.get_group(label).iloc[i][\"index\"]\n",
    "                if train_index == self.dt_indexes[\"query\"][label]: continue # クエリはselectedに含めない\n",
    "                selected_indexes.append(train_index)\n",
    "                self.dt_indexes[\"selected\"].append(train_index)\n",
    "        return self.dt_indexes, selected_indexes\n",
    "\n",
    "    # クエリを最遠傍点(FP)に更新\n",
    "    def update_FP_queries(self) -> Dict[str, List[int]]:\n",
    "        if len(self.faiss_indexes) is 0:\n",
    "            print(\"\\nPlease run the process to make faiss indexes in advance.\")\n",
    "            sys.exit()\n",
    "        dt_labelby = self.dropped_dt.groupby(\"label\")\n",
    "        for label in self.labels:\n",
    "            index = self.faiss_indexes[label]\n",
    "            k = index.ntotal # 検索対象データ数\n",
    "            query = self.dropped_dt[self.dropped_dt[\"index\"]==self.dt_indexes[\"query\"][label]][\"feature\"].iat[0] # queryに指定されたフィーチャを取得\n",
    "            query = json.loads(query)\n",
    "            query = np.array([query]).astype(\"float32\")\n",
    "            D, I = index.search(query, k)\n",
    "            for i in reversed(I[0]):\n",
    "                FP_query_index = dt_labelby.get_group(label).iloc[i][\"index\"]\n",
    "                if FP_query_index not in self.dt_indexes[\"selected_query\"]:\n",
    "                    break\n",
    "            self.dt_indexes[\"query\"][label] = FP_query_index\n",
    "        print(\"Updated query to Farthest point.\")\n",
    "        return self.dt_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Tuple, TypeVar, Dict\n",
    "\n",
    "Dataframe = TypeVar(\"pandas.core.frame.DataFrame\")\n",
    "Tensor = TypeVar(\"torch.Tensor\")\n",
    "NpArrayFloat32 = TypeVar(\"numpy.ndarray.float32\")\n",
    "NpInt64 = TypeVar(\"numpy.int64\")\n",
    "FaissIndexFlatL2 = TypeVar(\"faiss.swigfaiss.IndexFlatL2\")\n",
    "\n",
    "class FeatureSelector(DataSelector):\n",
    "    def __init__(self, ft: Dataframe, ft_indexes: Dict[str, Dict[int, List]]):\n",
    "        super().__init__(ft, ft_indexes)\n",
    "        \n",
    "    def __ft_to_features(self, ft: DataFrame) -> NpArrayFloat32:\n",
    "        features = [json.loads(f) for f in ft[\"feature\"]]\n",
    "        features = np.array(features).astype(\"float32\")\n",
    "        return features\n",
    "    \n",
    "    def __indexes_to_features(self, indexes: List[int]) -> NpArrayFloat32:\n",
    "        features = []\n",
    "        for index in indexes:\n",
    "            feature = self.dt[self.dt[\"index\"] == index][\"feature\"].iloc[0]\n",
    "            feature = json.loads(feature)\n",
    "            features.append(feature)\n",
    "        features = np.array(features).astype(\"float32\")\n",
    "        if len(features) != len(indexes): print(\"There is a feature that cannot be obtained\")\n",
    "        return features\n",
    "            \n",
    "    def __generate_faiss_index(self, vectors: NpArray) -> FaissIndexFlatL2:\n",
    "        dim = len(vectors[0])\n",
    "        faiss_index = faiss.IndexFlatL2(dim)\n",
    "        faiss_index.add(vectors)\n",
    "        return faiss_index\n",
    "    \n",
    "    def __search_NN_ft_indexes(self, queries: NpArray, faiss_index: FaissIndexFlatL2, dataN: int) -> List[int]:\n",
    "        k = faiss_index.ntotal # 検索対象データ数\n",
    "        D, I = faiss_index.search(queries, k) # 近傍探索\n",
    "        \n",
    "        NN_ft_indexes = []\n",
    "        query_indexes = [index for indexes in self.dt_indexes[\"queries\"].values() for index in indexes]\n",
    "        for indexes in I:\n",
    "            cnt, i = 0, 0\n",
    "            while cnt < dataN:\n",
    "                index = indexes[i]\n",
    "                i += 1\n",
    "                if index in query_indexes: continue # クエリは検索対象外\n",
    "                NN_ft_indexes.append(index)\n",
    "                cnt += 1\n",
    "                \n",
    "        return NN_ft_indexes\n",
    "                \n",
    "    def select_NN_ft_indexes(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "features = []\n",
    "for feature in ft[\"feature\"]:\n",
    "    features.append(json.loads(feature))\n",
    "features = np.array(features).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(features[0])\n",
    "faiss_index = faiss.IndexFlatL2(dim)\n",
    "faiss_index.add(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': {0: [1347, 2238, 2070, 8954, 4165],\n",
       "  1: [7522, 6188, 14867, 5218, 14887],\n",
       "  2: [916, 14267, 3882, 10627, 9697]},\n",
       " 'selected_data': {0: [], 1: [], 2: []},\n",
       " 'used_queries': {0: [], 1: [], 2: []}}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_indexes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for index in ft_indexes1[\"queries\"][0]:\n",
    "    query = ft[ft[\"index\"]==index][\"feature\"].iloc[0]\n",
    "    query = json.loads(query)\n",
    "    queries.append(query)\n",
    "queries = np.array(queries).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = faiss_index.ntotal # 検索対象データ数\n",
    "D, I = faiss_index.search(queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1347, 14108,  1180, ...,  1749,  5380,  8833],\n",
       "       [ 2238,  8587,   816, ...,  8787,  5957,  5810],\n",
       "       [ 2070, 14245,    94, ..., 10055,  5380,  8833],\n",
       "       [ 8954, 12173,   285, ..., 10055,  1609,  8833],\n",
       "       [ 4165,  2367,  6222, ..., 10055,  5380,  8833]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ft_indexes = []\n",
    "query_indexes = [index for indexes in ft_indexes1[\"queries\"].values() for index in indexes]\n",
    "dataN = 10\n",
    "for indexes in I:\n",
    "    cnt, i = 0, 0\n",
    "    while cnt < dataN:\n",
    "        index = indexes[i]\n",
    "        i += 1\n",
    "        if index in query_indexes: continue\n",
    "        NN_ft_indexes.append(index)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NN_ft_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
