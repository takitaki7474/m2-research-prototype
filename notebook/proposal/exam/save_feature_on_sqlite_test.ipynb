{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cifar10データセットをdownlaod_pathにダウンロード\n",
    "def load_cifar10(download_path):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    train = torchvision.datasets.CIFAR10(root=download_path, train=True, download=True, transform=transform)\n",
    "    test = torchvision.datasets.CIFAR10(root=download_path, train=False, download=True, transform=transform)\n",
    "    return train, test\n",
    "\n",
    "class DataSelector:\n",
    "\n",
    "    def __init__(self, train, test):\n",
    "        self.train = list(train)\n",
    "        self.test = list(test)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        dataset = (self.train, self.test)\n",
    "        return dataset\n",
    "\n",
    "    def print_len(self):\n",
    "        print(\"\\n------------------------------------------------------\")\n",
    "        print(\"訓練データ数:{0:>9}\".format(len(self.train)))\n",
    "        print(\"テストデータ数:{0:>9}\".format(len(self.test)))\n",
    "\n",
    "    def print_len_by_label(self):\n",
    "        texts = [\"訓練データ\", \"テストデータ\"]\n",
    "        for i, t in enumerate([self.train, self.test]):\n",
    "            dic = defaultdict(int)\n",
    "            for data in t:\n",
    "                dic[data[1]] += 1\n",
    "            print(\"\\n{0}数  ----------------------------------------------\".format(texts[i]))\n",
    "            dic = sorted(dic.items())\n",
    "            count = 0\n",
    "            for key, value in dic:\n",
    "                print(\"ラベル:  {0}    データ数:  {1}\".format(key, value))\n",
    "                count += value\n",
    "            print(\"合計データ数:  {0}\".format(count))\n",
    "\n",
    "    def randomly_select_data_by_label(self, data_num, train=True):\n",
    "        selected = []\n",
    "        dataset = self.train if train == True else self.test\n",
    "        dic = defaultdict(int)\n",
    "        for data in dataset:\n",
    "            if dic[data[1]] < data_num:\n",
    "                selected.append(data)\n",
    "                dic[data[1]] += 1\n",
    "        if train == True:\n",
    "            self.train = selected\n",
    "        else:\n",
    "            self.test = selected\n",
    "\n",
    "    def __select_data_by_label(self, label):\n",
    "        selected = [[], []]\n",
    "        dataset = (self.train, self.test)\n",
    "        for i, t in enumerate(dataset):\n",
    "            for data in t:\n",
    "                if data[1] == label:\n",
    "                    selected[i].append(data)\n",
    "        return selected\n",
    "\n",
    "    def select_data_by_labels(self, labels):\n",
    "        selected = [[], []]\n",
    "        for label in labels:\n",
    "            result = self.__select_data_by_label(label)\n",
    "            selected[0] += result[0]\n",
    "            selected[1] += result[1]\n",
    "        self.train, self.test = selected\n",
    "\n",
    "    def update_labels(self):\n",
    "        updated = [[], []]\n",
    "        label_mapping = defaultdict(lambda: -1)\n",
    "        for i, t in enumerate([self.train, self.test]):\n",
    "            t = sorted(t, key=lambda x:x[1])\n",
    "            new_label = 0\n",
    "            for data in t:\n",
    "                if label_mapping[data[1]] == -1:\n",
    "                    label_mapping[data[1]] = new_label\n",
    "                    new_label += 1\n",
    "                updated[i].append((data[0], label_mapping[data[1]]))\n",
    "        self.train, self.test = updated\n",
    "        print(\"\\nラベルを変更しました.  ----------------------------------------------\")\n",
    "        for key, value in label_mapping.items():\n",
    "            print(\"ラベル  {0} -> {1}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "ラベルを変更しました.  ----------------------------------------------\n",
      "ラベル  1 -> 0\n",
      "ラベル  2 -> 1\n",
      "ラベル  8 -> 2\n",
      "\n",
      "訓練データ数  ----------------------------------------------\n",
      "ラベル:  0    データ数:  300\n",
      "ラベル:  1    データ数:  300\n",
      "ラベル:  2    データ数:  300\n",
      "合計データ数:  900\n",
      "\n",
      "テストデータ数  ----------------------------------------------\n",
      "ラベル:  0    データ数:  30\n",
      "ラベル:  1    データ数:  30\n",
      "ラベル:  2    データ数:  30\n",
      "合計データ数:  90\n"
     ]
    }
   ],
   "source": [
    "download_path = \"./data/\"\n",
    "train, test = load_cifar10(download_path)\n",
    "data_selector = DataSelector(train, test)\n",
    "data_selector.select_data_by_labels([1, 2, 8])\n",
    "data_selector.randomly_select_data_by_label(300, train=True)\n",
    "data_selector.randomly_select_data_by_label(30, train=False)\n",
    "data_selector.update_labels()\n",
    "data_selector.print_len_by_label()\n",
    "train, test = data_selector.get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------- フィーチャ抽出とsqliteへの保存 --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PreLeNet(nn.Module):\n",
    "    def __init__(self, out=3):\n",
    "        super(PreLeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1) # (1) 32*32*3 -> 32*32*16\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1) # (3) 16*16*16 -> 16*16*32\n",
    "        self.gap = nn.AvgPool2d(kernel_size=8)\n",
    "        self.fc1 = nn.Linear(8*8*32, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # (2) 32*32*16 -> 16*16*16\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # (4) 16*16*32 -> 8*8*32\n",
    "        feature = self.gap(x) # 1*1*32\n",
    "        feature = feature.view(-1, 32) # 1*32\n",
    "        x = x.view(-1, 8*8*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------- 通常保存------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "model_path = \"./v3.pth\"\n",
    "net = PreLeNet(3)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "dbname = \"features.db\"\n",
    "conn = sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "c.execute(\"create table featuretable (id integer PRIMARY KEY, label integer, image text, feature text)\")\n",
    "sql = \"insert into featuretable (id, label, image, feature) values (?, ?, ?, ?)\"\n",
    "\n",
    "id = 1\n",
    "for i, (inputs, labels) in enumerate(dataloader):\n",
    "    outputs, features = net(inputs)\n",
    "    for label, image, feature in zip(labels, inputs, features):\n",
    "        label = int(label)\n",
    "        image = json.dumps(image.cpu().numpy().tolist())\n",
    "        feature = json.dumps(feature.data.cpu().numpy().tolist())\n",
    "        data = (id, label, image, feature)\n",
    "        c.execute(sql, data)\n",
    "        id += 1\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"features.db\"\n",
    "conn = sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "sql = \"select * from featuretable\"\n",
    "c.execute(sql)\n",
    "data = c.fetchone()\n",
    "id = data[0]\n",
    "label = data[1]\n",
    "image = json.loads(data[2])\n",
    "feature = json.loads(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------------------------- DataFrame保存 ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "model_path = \"./v3.pth\"\n",
    "net = PreLeNet(3)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "dbname = \"features_dataframe.db\"\n",
    "conn = sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "d = defaultdict(list)\n",
    "for i, (inputs, labels) in enumerate(dataloader):\n",
    "    outputs, features = net(inputs)\n",
    "    for label, image, feature in zip(labels, inputs, features):\n",
    "        d[\"label\"].append(int(label))\n",
    "        d[\"image\"].append(json.dumps(image.cpu().numpy().tolist()))\n",
    "        d[\"feature\"].append(json.dumps(feature.data.cpu().numpy().tolist()))\n",
    "\n",
    "for k, v in d.items():\n",
    "    d[k] = pd.Series(v)\n",
    "    \n",
    "df = pd.DataFrame(d)\n",
    "df.to_sql('feature_table', conn, if_exists='replace')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"features_dataframe.db\"\n",
    "conn=sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "df = pd.read_sql('SELECT * FROM feature_table', conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------- sqlite 保存形式の確認 ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"test.db\"\n",
    "conn = sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "c.execute(\"create table test (id integer, array text)\")\n",
    "\n",
    "sql = 'insert into test (id, array) values (?,?)'\n",
    "x = np.arange(12).reshape(2,6)\n",
    "data = (1, json.dumps(x.tolist()))\n",
    "c.execute(sql, data)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "dbname = \"test.db\"\n",
    "conn = sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "sql = \"select * from test\"\n",
    "c.execute(sql)\n",
    "data = c.fetchall()\n",
    "result_list = json.loads(data[0][1])\n",
    "print(type(result_list))\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------ sqlite3 test ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'test.db'\n",
    "\n",
    "conn=sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "# executeメソッドでSQL文を実行する\n",
    "create_table = 'create table sample (id integer,name text)'\n",
    "c.execute(create_table)\n",
    "\n",
    "# SQL文に値をセットする場合は，Pythonのformatメソッドなどは使わずに，\n",
    "# セットしたい場所に?を記述し，executeメソッドの第2引数に?に当てはめる値をタプルで渡す．\n",
    "sql = 'insert into sample (id, name) values (?,?)'\n",
    "text = \"test test test\"\n",
    "query = (1, text)\n",
    "c.execute(sql, query)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'test test test')]\n"
     ]
    }
   ],
   "source": [
    "dbname = 'test.db'\n",
    "\n",
    "conn=sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "sql = 'select * from sample'\n",
    "c.execute(sql)\n",
    "result=c.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------- DataFrame をsqliteに保存 --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"pdtest.db\"\n",
    "conn=sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "\n",
    "d = {}\n",
    "d[0] = \"aaa\"\n",
    "d[1] = \"bbb\"\n",
    "d[2] = \"ccc\"\n",
    "\n",
    "df = pd.DataFrame(list(d.items()), columns=[\"id\", \"text\"])\n",
    "\n",
    "df.to_sql('sample', conn, if_exists='replace')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  id text\n",
      "0      0   0  aaa\n",
      "1      1   1  bbb\n",
      "2      2   2  ccc\n"
     ]
    }
   ],
   "source": [
    "dbname = \"pdtest.db\"\n",
    "conn=sqlite3.connect(dbname)\n",
    "c = conn.cursor()\n",
    "df = pd.read_sql('SELECT * FROM sample', conn)\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
